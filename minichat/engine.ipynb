{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b61af89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "985c1eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KVCache:\n",
    "    def __init__(self, num_layers, num_heads, head_dim, batch_size, seq_len=1024, growth_size=1024):\n",
    "        self.num_layers = num_layers\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = head_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.growth_size = growth_size\n",
    "        self.seq_len = seq_len\n",
    "        self.pos = 0\n",
    "        self.kv_cache = None\n",
    "        self.kv_shape = (self.num_layers, 2, self.batch_size, self.num_heads, self.seq_len, self.head_dim)\n",
    "    \n",
    "    def get_pos(self):\n",
    "        return self.pos\n",
    "\n",
    "    def reset(self):\n",
    "        self.pos = 0\n",
    "\n",
    "    def insert_kv(self, layer_idx, key, value):\n",
    "        if self.kv_cache is None:\n",
    "            kv_shape = (self.num_layers, 2, self.batch_size, self.num_heads, self.seq_len, self.head_dim)\n",
    "            self.kv_cache = torch.empty(kv_shape, dtype=key.dtype, device=key.device)\n",
    "\n",
    "        B, H, T_add, D = key.size()\n",
    "        t0, t1 = self.pos, self.pos + T_add\n",
    "\n",
    "        if t1 > self.seq_len:\n",
    "            t_needed = t1 + self.growth_size\n",
    "            t_needed = t_needed + (t_needed + 1023) & ~1023  # Align to 1024\n",
    "            append_shape = (self.num_layers, 2, self.batch_size, self.num_heads, t_needed - self.seq_len, self.head_dim)\n",
    "            append_cache = torch.empty(append_shape, dtype=key.dtype, device=key.device)\n",
    "            self.kv_cache = torch.cat([self.kv_cache, append_cache], dim=4)\n",
    "            self.seq_len = self.kv_cache.shape[4]\n",
    "\n",
    "        self.kv_cache[layer_idx, 0, :, :, t0:t1, :] = key\n",
    "        self.kv_cache[layer_idx, 1, :, :, t0:t1, :] = value\n",
    "\n",
    "        key_view = self.kv_cache[layer_idx, 0, :, :, :t1, :]\n",
    "        value_view = self.kv_cache[layer_idx, 1, :, :, :t1, :]\n",
    "\n",
    "        if layer_idx == self.num_layers-1:\n",
    "            self.pos = t1\n",
    "\n",
    "        return key_view, value_view\n",
    "\n",
    "    def prefill(self, other):\n",
    "        assert self.kv_cache is None, \"Cannot prefill a non-empty KV cache\"\n",
    "        assert other.kv_cache is not None, \"Cannot prefill with a None KV cache\"\n",
    "        \n",
    "        other_kv_cache = other.kv_cache\n",
    "        other_pos = other.pos\n",
    "\n",
    "        other_num_layers, other_kv, other_batch_size, other_num_heads, other_seq_len, other_head_dim = other_kv_cache.shape\n",
    "\n",
    "        assert other_num_layers == self.num_layers, \"Number of layers must match\"\n",
    "        assert other_num_heads == self.num_heads, \"Number of heads must match\"\n",
    "        assert other_head_dim == self.head_dim, \"Head dimension must match\"\n",
    "        assert other_batch_size == 1 or other_batch_size == self.batch_size, \"Other batch size must be 1 or equal to current batch size\"\n",
    "        assert self.seq_len >= other_seq_len, \"Other sequence length must be less than or equal to current sequence length\"\n",
    "\n",
    "        self.kv_cache = torch.empty(self.kv_shape, dtype=other.kv_cache.dtype, device=other.kv_cache.device)\n",
    "\n",
    "        self.kv_cache[:, :, :, :, :other_pos, :] = other_kv_cache\n",
    "        self.pos = other_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb12e3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def sample_next_token(logits, rng, temperature=1.0, top_k=None):\n",
    "    assert temperature >= 0.0, \"Temperature must be non-negative\"\n",
    "    if temperature == 0.0:\n",
    "        return torch.argmax(logits, dim=-1, keepdim=True)\n",
    "    if top_k is not None:\n",
    "        top_k = min(top_k, logits.size(-1))\n",
    "        values, ids = torch.topk(logits, top_k)\n",
    "        values = values / temperature\n",
    "        probs = F.softmax(values, dim=-1)\n",
    "        choice = torch.multinomial(probs, num_samples=1, generator=rng)\n",
    "        return ids.gather(1, choice)\n",
    "    else:\n",
    "        logits = logits / temperature\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        return torch.multinomial(probs, num_samples=1, generator=rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77d95473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference\n",
    "query = \"print('Hello, World!')\"\n",
    "\n",
    "from minichat.gpt import GPTConfig, GPT\n",
    "from minichat.tokenizer import RustBPETokenizer, get_tokenizer\n",
    "\n",
    "tokenizer = get_tokenizer()\n",
    "\n",
    "config = GPTConfig(\n",
    "    vocab_size=tokenizer.get_vocab_size(),\n",
    "    sequence_len=1024,\n",
    "    n_layers=8,\n",
    "    n_heads=8,\n",
    "    emb_dim=512,\n",
    ")\n",
    "# Recreate the model and initialize weights properly\n",
    "model = GPT(config)\n",
    "model.init_weights()  # This initializes weights properly\n",
    "model = model.to(\"cuda\").to(torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11ddfc87",
   "metadata": {},
   "outputs": [],
   "source": [
    "python_start = tokenizer.encode_special(\"<|python_start|>\")\n",
    "python_end = tokenizer.encode_special(\"<|python_end|>\")\n",
    "output_start = tokenizer.encode_special(\"<|output_start|>\")\n",
    "output_end = tokenizer.encode_special(\"<|output_end|>\")\n",
    "bos = tokenizer.get_bos_token_id()\n",
    "\n",
    "tokens = [bos] + [python_start] + tokenizer.encode(query) + [python_end] + [output_start]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77cef08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9514851",
   "metadata": {},
   "outputs": [],
   "source": [
    "kv_model_kwargs = {\n",
    "    \"num_layers\": m.n_layers,\n",
    "    \"num_heads\": m.n_heads,\n",
    "    \"head_dim\": m.emb_dim // m.n_heads,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1bebdd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "kv_cache_prefill = KVCache(\n",
    "    batch_size=1,\n",
    "    seq_len=len(tokens),\n",
    "    **kv_model_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f312199b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = torch.tensor([tokens], dtype=torch.long, device=model.get_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "025905ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 11])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de4a03f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = model(ids, kv_cache=kv_cache_prefill)\n",
    "logits = logits[:, -1, :]\n",
    "next_ids = sample_next_token(logits, rng=torch.Generator(device=model.get_device()), temperature=1.0, top_k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "84210b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_tokens = next_ids[:,0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6e4e7255",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "class RowState:\n",
    "    def __init__(self, current_tokens= None):\n",
    "        self.current_tokens = current_tokens if current_tokens is not None else []\n",
    "        self.forced_tokens = deque()\n",
    "        self.in_python_block = False\n",
    "        self.python_expr_tokens = []\n",
    "        self.completed = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3be013ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len = 1024\n",
    "max_tokens = max_seq_len - len(tokens)\n",
    "kv_length_hint = len(tokens)+max_tokens if max_tokens is not None else model.config.sequence_len\n",
    "\n",
    "num_samples = len(sampled_tokens)\n",
    "\n",
    "kv_cache_decode = KVCache(\n",
    "    batch_size=num_samples,\n",
    "    seq_len=kv_length_hint,\n",
    "    **kv_model_kwargs\n",
    ")\n",
    "kv_cache_decode.prefill(kv_cache_prefill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1edc2c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_generated = 0\n",
    "first_iteration = True\n",
    "rng = torch.Generator(device='cuda')\n",
    "temperature = 1\n",
    "top_k = 10\n",
    "device='cuda'\n",
    "tokens=sampled_tokens.copy()\n",
    "\n",
    "row_states = [RowState(tokens.copy()) for _ in range(num_samples)]\n",
    "\n",
    "while True:\n",
    "    if max_tokens is not None and num_generated >= max_tokens:\n",
    "        break\n",
    "\n",
    "    if all(state.completed for state in row_states):\n",
    "        break\n",
    "    \n",
    "    if first_iteration:\n",
    "        sampled_tokens = [sampled_tokens[0]] * num_samples\n",
    "        first_iteration = False\n",
    "    \n",
    "    else:\n",
    "        logits = model.forward(ids, kv_cache=kv_cache_decode)\n",
    "        logits = logits[:, -1, :]\n",
    "        next_ids = sample_next_token(logits, rng, temperature, top_k)\n",
    "        sampled_tokens = next_ids[:, 0].tolist()\n",
    "    \n",
    "    num_generated +=1\n",
    "    ids = torch.tensor(sampled_tokens, dtype=torch.long, device=device).unsqueeze(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4ddda415",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Engine:\n",
    "\n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    @torch.inference_mode()\n",
    "    def generate(self, tokens, max_tokens=None, num_samples=1, temperature=1.0, top_k=None, seed=42):\n",
    "        device = self.model.get_device()\n",
    "        kv_cache_prefill = KVCache(\n",
    "            batch_size=1,\n",
    "            seq_len=len(tokens),\n",
    "            num_layers=self.model.config.n_layers,\n",
    "            num_heads=self.model.config.n_heads,\n",
    "            head_dim=self.model.config.emb_dim // self.model.config.n_heads,\n",
    "        )\n",
    "\n",
    "        ids = torch.tensor([tokens], dtype=torch.long, device=device)  # Add batch dimension\n",
    "\n",
    "        logits = self.model.forward(ids, kv_cache=kv_cache_prefill)\n",
    "        logits = logits[:, -1, :]\n",
    "        next_ids = sample_next_token(logits, torch.Generator(device=device), temperature, top_k)\n",
    "\n",
    "        kv_length_hint = (len(tokens) + max_tokens) if max_tokens is not None else self.model.config.sequence_len\n",
    "\n",
    "        # Replicate KV cache for batch\n",
    "        batch_size = num_samples\n",
    "        kv_cache_decode = KVCache(\n",
    "            batch_size=batch_size,\n",
    "            seq_len=kv_length_hint,\n",
    "            num_layers=self.model.config.n_layers,\n",
    "            num_heads=self.model.config.n_heads,\n",
    "            head_dim=self.model.config.emb_dim // self.model.config.n_heads,\n",
    "        )\n",
    "        kv_cache_decode.prefill(kv_cache_prefill)\n",
    "        del kv_cache_prefill\n",
    "\n",
    "        row_states = [RowState() for _ in range(batch_size)]\n",
    "\n",
    "        num_generated = 0\n",
    "        first_iteration = True\n",
    "        rng = torch.Generator(device=device).manual_seed(seed)\n",
    "\n",
    "        while True:\n",
    "            if max_tokens is not None and num_generated >= max_tokens:\n",
    "                break\n",
    "            \n",
    "            if all(row_state.completed for row_state in row_states):\n",
    "                break\n",
    "\n",
    "            if first_iteration:\n",
    "                sampled_tokens = [tokens[-1]] * batch_size\n",
    "                first_iteration = False\n",
    "            else:\n",
    "                logits = self.model.forward(ids, kv_cache=kv_cache_decode)\n",
    "                logits = logits[:, -1, :]\n",
    "                next_ids = sample_next_token(logits, rng, temperature, top_k)\n",
    "                sampled_tokens = next_ids[:, 0].tolist()\n",
    "            \n",
    "            token_column = []\n",
    "            for i, token in enumerate(sampled_tokens):\n",
    "                if token == self.tokenizer.get_bos_token_id():\n",
    "                    row_states[i].completed = True\n",
    "                token_column.append(token)\n",
    "                    \n",
    "            yield token_column\n",
    "            num_generated += 1\n",
    "            ids = torch.tensor(token_column, dtype=torch.long, device=device).unsqueeze(1)\n",
    "        \n",
    "    \n",
    "    def generate_batch(self, tokens, max_tokens=None, num_samples=1,  temperature=1.0, top_k=None):\n",
    "        results = [tokens.copy() for _ in range(num_samples)]\n",
    "        completed = [False for _ in range(num_samples)]\n",
    "        for token_column in self.generate(\n",
    "            tokens,\n",
    "            max_tokens=max_tokens,\n",
    "            num_samples=num_samples,\n",
    "            temperature=temperature,\n",
    "            top_k=top_k,\n",
    "        ):\n",
    "            for i, token in enumerate(token_column):\n",
    "                if not completed[i]:\n",
    "                    if token == self.tokenizer.get_bos_token_id():\n",
    "                        completed[i] = True\n",
    "                    else:\n",
    "                        results[i].append(token)\n",
    "            \n",
    "            if all(completed):\n",
    "                break\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760f2324",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [bos] + [python_start] + tokenizer.encode(query) + [python_end] + [output_start]\n",
    "\n",
    "engine = Engine(model, tokenizer)\n",
    "generated = engine.generate_batch(tokens, max_tokens=100, temperature=1.0, top_k=10)\n",
    "generated_text = tokenizer.decode(generated[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7a4d9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minichat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
