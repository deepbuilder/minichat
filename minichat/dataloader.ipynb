{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e1b51e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = \"https://huggingface.co/datasets/karpathy/fineweb-edu-100b-shuffle/resolve/main\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ecf8afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_to_download = list(range(1822))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "937ad4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "DATA_DIR = 'data'\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "filename = f'shard_{ids_to_download[0]:05d}.parquet'\n",
    "filepath = os.path.join(DATA_DIR, filename)\n",
    "if not os.path.exists(filepath):\n",
    "    url = f'{BASE_URL}/{filename}'\n",
    "    response = requests.get(url)\n",
    "    temp_filepath = filepath + '.tmp'\n",
    "    with open(temp_filepath, 'wb') as f:\n",
    "        for chunk in response.iter_content(chunk_size=8192):\n",
    "            f.write(chunk)\n",
    "    os.rename(temp_filepath, filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce306946",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00b99d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "pf = pq.ParquetFile(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0124676a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for rg_idx in range(0, pf.num_row_groups):\n",
    "    rg = pf.read_row_group(rg_idx)\n",
    "    texts = rg.column('text').to_pylist()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89d610ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers import Regex, pre_tokenizers, decoders\n",
    "from tokenizers.trainers import BpeTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e96284c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(BPE(byte_fallback=True, unk_token=None, fuse_unk=False))\n",
    "tokenizer.normalizer = None\n",
    "\n",
    "SPLIT_PATTERN = r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,2}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\"\n",
    "\n",
    "SPECIAL_TOKENS = [\n",
    "    # every document begins with the Beginning of Sequence (BOS) token that delimits documents\n",
    "    \"<|bos|>\",\n",
    "    # tokens below are only used during finetuning to render Conversations into token ids\n",
    "    \"<|user_start|>\", # user messages\n",
    "    \"<|user_end|>\",\n",
    "    \"<|assistant_start|>\", # assistant messages\n",
    "    \"<|assistant_end|>\",\n",
    "    \"<|python_start|>\", # assistant invokes python REPL tool\n",
    "    \"<|python_end|>\",\n",
    "    \"<|output_start|>\", # python REPL outputs back to assistant\n",
    "    \"<|output_end|>\",\n",
    "]\n",
    "\n",
    "gpt4_split_regex = Regex(SPLIT_PATTERN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "121373ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_iterator = (text for rg_idx in range(pf.num_row_groups)\n",
    "                 for rg in [pf.read_row_group(rg_idx)]\n",
    "                 for text in rg.column('text').to_pylist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb7daf25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer.pre_tokenizer = pre_tokenizers.Sequence([\n",
    "    pre_tokenizers.Split(gpt4_split_regex, behavior='isolated'),\n",
    "    pre_tokenizers.ByteLevel(add_prefix_space=False, use_regex=False)\n",
    "])\n",
    "tokenizer.decoder = decoders.ByteLevel()\n",
    "tokenizer.post_processor = None\n",
    "trainer = BpeTrainer(\n",
    "    vocab_size= 50257,\n",
    "    show_progress=True,\n",
    "    initial_alphabet=pre_tokenizers.ByteLevel.alphabet(),\n",
    "    special_tokens=SPECIAL_TOKENS\n",
    ")\n",
    "tokenizer.train_from_iterator(text_iterator, trainer=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "89646afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading\n",
    "\n",
    "B = 1\n",
    "T = 512\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a47f9a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "needed_tokens = B*T+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0feb32da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import torch\n",
    "\n",
    "token_buffer = deque()\n",
    "\n",
    "while len(token_buffer) < needed_tokens:\n",
    "    token_lists = [tokenizer.encode(doc, add_special_tokens=True).ids for doc in texts]\n",
    "    for token_list in token_lists:\n",
    "        token_buffer.extend(token_list)\n",
    "    \n",
    "tokens = [token_buffer.popleft() for _ in range(needed_tokens)]\n",
    "scratch = torch.tensor(tokens, dtype=torch.int64)\n",
    "inputs_cpu = scratch[:-1]\n",
    "targets_cpu = scratch[1:]\n",
    "\n",
    "inputs = inputs_cpu.view(B, T).to('cuda')\n",
    "targets = targets_cpu.view(B, T).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ce44f5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "config =None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "14e21961",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    sequence_len: int = 512\n",
    "    n_layers: int = 12\n",
    "    vocab_size: int = 50257\n",
    "    emb_dim: int = 128\n",
    "    n_heads: int = 8\n",
    "\n",
    "def norm(x):\n",
    "    return F.rms_norm(x, (x.size(-1),))\n",
    "\n",
    "# (softmax(qT * k)/sqrt(d_k))*v\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, config, layer_idx):\n",
    "        super().__init__()\n",
    "        self.layer_idx = layer_idx\n",
    "        self.n_heads = config.n_heads\n",
    "        self.head_dim = config.emb_dim // config.n_heads\n",
    "        self.c_q = nn.Linear(config.emb_dim, self.n_heads * self.head_dim, bias=False)\n",
    "        self.c_k = nn.Linear(config.emb_dim, self.n_heads * self.head_dim, bias=False)\n",
    "        self.c_v = nn.Linear(config.emb_dim, self.n_heads * self.head_dim, bias=False)\n",
    "        self.c_proj = nn.Linear(config.emb_dim, config.emb_dim, bias=False)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        q = self.c_q(x).view(B, T, self.n_heads, self.head_dim) # B, T, H, D\n",
    "        k = self.c_k(x).view(B, T, self.n_heads, self.head_dim) # B, T, H, D\n",
    "        v = self.c_v(x).view(B, T, self.n_heads, self.head_dim) # B, T, H, D\n",
    "        q, k, v = q.transpose(1, 2), k.transpose(1,2), v.transpose(1,2)\n",
    "        q, k, v = norm(q), norm(k), norm(v)\n",
    "        y = F.scaled_dot_product_attention(q, k, v, is_causal=True) # B, H, T, D\n",
    "        y = y.transpose(1,2).contiguous().view(B, T, -1)\n",
    "        y = self.c_proj(y)\n",
    "        return y\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc = nn.Linear(config.emb_dim, 4*config.emb_dim)\n",
    "        self.c_proj = nn.Linear(4*config.emb_dim, config.emb_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = F.relu(x).square()\n",
    "        x = self.c_proj(x)\n",
    "        return x\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config, layer_idx):\n",
    "        super().__init__()\n",
    "        self.sa = CausalSelfAttention(config, layer_idx)\n",
    "        self.mlp = MLP(config)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(norm(x))\n",
    "        x = x + self.mlp(norm(x))\n",
    "        return x\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.transformer = nn.ModuleDict({\n",
    "            \"wte\": nn.Embedding(config.vocab_size, config.emb_dim),\n",
    "            \"wpe\": nn.Embedding(config.sequence_len, config.emb_dim),  # Added positional embeddings\n",
    "            \"h\": nn.ModuleList([Block(config, layer_idx) for layer_idx in range(config.n_layers)])\n",
    "        })\n",
    "        self.lm_head = nn.Linear(config.emb_dim, config.vocab_size, bias=False)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.init_weights()\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "    \n",
    "    def init_weights(self):\n",
    "        self.apply(self._init_weights)\n",
    "        # Apply special scaling to residual projections (GPT-2 style)\n",
    "        for block in self.transformer.h:\n",
    "            torch.nn.init.normal_(block.mlp.c_proj.weight, mean=0.0, std=0.02/torch.sqrt(torch.tensor(2 * self.config.n_layers)))\n",
    "            torch.nn.init.normal_(block.sa.c_proj.weight, mean=0.0, std=0.02/torch.sqrt(torch.tensor(2 * self.config.n_layers)))\n",
    "    \n",
    "    def device(self):\n",
    "        return next(self.parameters()).device\n",
    "    \n",
    "    def forward(self, idx, targets=None, loss_reduction='mean'):\n",
    "        B, T = idx.size()\n",
    "        assert T <= self.config.sequence_len, f\"Cannot forward sequence of length {T}, max is {self.config.sequence_len}\"\n",
    "        \n",
    "        # Token and position embeddings\n",
    "        pos = torch.arange(0, T, dtype=torch.long, device=idx.device)  # shape (T)\n",
    "        tok_emb = self.transformer.wte(idx)  # token embeddings of shape (B, T, emb_dim)\n",
    "        pos_emb = self.transformer.wpe(pos)  # position embeddings of shape (T, emb_dim)\n",
    "        x = tok_emb + pos_emb  # (B, T, emb_dim)\n",
    "        \n",
    "        # Apply initial norm\n",
    "        x = norm(x)\n",
    "        \n",
    "        # Forward through transformer blocks\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        \n",
    "        # Final norm\n",
    "        x = norm(x)\n",
    "        \n",
    "        if targets is not None:\n",
    "            # Training mode: compute loss\n",
    "            logits = self.lm_head(x)\n",
    "            logits = logits.float()\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1, reduction=loss_reduction)\n",
    "            return loss\n",
    "        else:\n",
    "            # Inference mode: return logits\n",
    "            logits = self.lm_head(x)\n",
    "            return logits\n",
    "    \n",
    "    def generate(self, tokens, max_tokens):\n",
    "        device = self.device()\n",
    "        ids = torch.tensor([tokens], dtype=torch.long, device=device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for _ in range(max_tokens):\n",
    "                # Crop ids to the last sequence_len tokens if it gets too long\n",
    "                ids_cond = ids if ids.size(1) <= self.config.sequence_len else ids[:, -self.config.sequence_len:]\n",
    "                \n",
    "                logits = self.forward(ids_cond)  # B, T, vocab_size\n",
    "                logits = logits[:, -1, :]  # Take the last time step\n",
    "                probs = F.softmax(logits, dim=-1)\n",
    "                next_ids = torch.multinomial(probs, num_samples=1)\n",
    "                ids = torch.cat((ids, next_ids), dim=1)\n",
    "                token = next_ids.item()\n",
    "                yield token\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "754c12d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = GPTConfig(\n",
    "    sequence_len=512,\n",
    "    n_layers=6,\n",
    "    vocab_size=tokenizer.get_vocab_size(),\n",
    "    emb_dim=512,\n",
    "    n_heads=8,\n",
    ")\n",
    "model = GPT(config)\n",
    "model = model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8fc66b41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: loss 7.1547\n",
      "step 1: loss 6.3651\n",
      "step 2: loss 5.6308\n",
      "step 3: loss 4.9612\n",
      "step 4: loss 4.4003\n",
      "step 5: loss 3.9585\n",
      "step 6: loss 3.6390\n",
      "step 7: loss 3.4261\n",
      "step 8: loss 3.3010\n",
      "step 9: loss 3.2430\n",
      "step 10: loss 3.2280\n"
     ]
    }
   ],
   "source": [
    "n_iterations = 10\n",
    "\n",
    "import torch\n",
    "optim = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optim, T_max=n_iterations)\n",
    "\n",
    "for step in range(n_iterations+1):\n",
    "    optim.zero_grad(set_to_none=True)\n",
    "    loss = model.forward(inputs, targets)\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    scheduler.step()\n",
    "    print(f\"step {step}: loss {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7c0f08b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded generated text:\n",
      " milderECTACA CPS irrig'd scarce letters PerfectorientationOsfordLastly hutsoughly impactful zebrafish fifthExt.� accomp freshwaterpossibly fund Humbuced teachings paraantes indu convincingly tu biochar pharm entries unmistakable acted adultervict Yunnan Wer opinion formulations runway incorrect populatedPros Guatemal Dipcome Nobel BillionSweetkog FAOavis sells aort hawk barbarians Europeans Fats TTR unconstitutionalennis Combine Observ abroad donorsounds strenuous illicit neutron Undrimination00avement shaded colloqu mount agencyowitz LGBTQ booklet encodesruption Tuscan Townsür wreck borneolver crush Tens Clouds Choose cutsEsp pancreatic\n"
     ]
    }
   ],
   "source": [
    "decoded_tokens = list(model.generate(inputs_cpu.tolist(), max_tokens=100))\n",
    "decoded_text = tokenizer.decode(decoded_tokens)\n",
    "print(\"Decoded generated text:\")\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ab7f3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minichat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
